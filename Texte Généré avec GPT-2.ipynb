{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16719bab-8887-4e9e-92d4-90aeb61c6ae9",
   "metadata": {},
   "source": [
    "# Chargement du modèle GPT-2 pré-entraîné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f3416d0-100b-4747-add6-d58e7f89316e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aya\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Add padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b6f12f-4cd8-4910-9674-04821d004616",
   "metadata": {},
   "source": [
    "# Préparer le texte d'entrée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a2d25cc-cf3d-4dc2-a201-98dba26156c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"The future of AI is\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')  # Convert text to input tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58589aed-20d4-429e-ac80-49ca4552f4f6",
   "metadata": {},
   "source": [
    "# Générer du texte "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879af883-e80c-4c05-8ca2-f022f3337ede",
   "metadata": {},
   "source": [
    "En utilisant la méthode *generate*, pour produire un nouveau texte basé sur l'entrée donnée.\n",
    "    \n",
    "    - max_length : Limite la longueur du texte généré.\n",
    "    - temperature : Ajuste la créativité.\n",
    "    - top_k : Restreint l'échantillonnage aux k mots les plus probables.\n",
    "    - top_p : Sélectionne les mots les plus probables jusqu'à atteindre une certaine probabilité cumulative, pour un texte plus diversifié.\n",
    "    - repetition_penalty : Réduit les répétitions excessives dans le texte généré."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d02b9232-a0b0-48b4-a4f1-a3cd246d3ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: \n",
      " The future of AI is a big one. If we can create machines that are intelligent, not just smart but also capable of taking care of their own needs and giving them the best possible life they want – then this will happen in two decades' time\n"
     ]
    }
   ],
   "source": [
    "# Generate text\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,        \n",
    "    num_return_sequences=1,  \n",
    "    temperature=0.7,        \n",
    "    top_k=50,               \n",
    "    top_p=0.95,             # Nucleus sampling for more focused results\n",
    "    repetition_penalty=1.2, # Penalize repeated tokens to avoid loops\n",
    "    do_sample=True,         \n",
    ")\n",
    "\n",
    "# Decode generated output\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated Text: \\n\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb25f65-d6d2-458c-b03b-9dd73dcbd766",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
